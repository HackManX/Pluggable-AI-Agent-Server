<div align="center">üöÄ Pluggable AI Agent Server üöÄ(Gemini Edition)</div><p align="center">A robust, context-aware, and extensible conversational AI server built with TypeScript and powered by the Google Gemini API. This project fulfills the requirements of the internship technical assignment by implementing a multi-step reasoning agent with memory, RAG, and a dynamic plugin system.</p><p align="center"><img src="https://img.shields.io/badge/TypeScript-3178C6?style=for-the-badge&logo=typescript&logoColor=white" alt="TypeScript Badge"/><img src="https://img.shields.io/badge/Node.js-339933?style=for-the-badge&logo=nodedotjs&logoColor=white" alt="Node.js Badge"/><img src="https://img.shields.io/badge/Express.js-000000?style=for-the-badge&logo=express&logoColor=white" alt="Express.js Badge"/><img src="https://img.shields.io/badge/Google%20Gemini-8E75B2?style=for-the-badge&logo=google-gemini&logoColor=white" alt="Gemini Badge"/></p>üìç Live DemoThe agent is deployed and live on Render. You can test the endpoint directly:Live URL: https://pluggable-ai-agent-server.onrender.com‚ú® Core FeaturesFeatureDescriptionüß† Session-Based MemoryMaintains conversation history on a per-session basis, allowing for natural, context-aware follow-up questions.üìö Retrieval-Augmented Generation (RAG)Uses an in-memory vector store built with Gemini embeddings to retrieve relevant information from a local knowledge base (.md files) to inform its answers.üîå Plugin System (Tool Use)Intelligently decides when to use external tools. It currently supports a Weather Plugin and a Math Plugin.üèóÔ∏è System ArchitectureThe agent processes requests using a defined, multi-step flow:Request In ‚û°Ô∏è Session Management ‚û°Ô∏è RAG Retrieval ‚û°Ô∏è Context Assembly ‚û°Ô∏è LLM Orchestration (Tool Use) ‚û°Ô∏è Plugin Execution ‚û°Ô∏è Final Response Generation ‚û°Ô∏è Response Out<details><summary><strong>Click to expand the detailed workflow</strong></summary>Request Handling: The Express server receives a POST request containing a message and a session_id.Session Management: A ChatSession object from the Gemini SDK is retrieved or created for the given session_id, ensuring conversation history is maintained.RAG Retrieval: The user's message is embedded using the Gemini embeddings model. This embedding is used to query the in-memory vector store via cosine similarity, retrieving the top 3 most relevant text chunks.Context Assembly: The retrieved RAG chunks are prepended to the user's message to form a rich, context-aware prompt.LLM Orchestration (Tool Use): The prompt is sent to the Gemini 1.5 Pro model, which has been initialized with definitions of the available tools. It analyzes the prompt and decides whether to respond directly or to call a function.Plugin Execution: If Gemini returns a functionCall, the server executes the corresponding local plugin function (e.g., getWeather()).Final Response Generation: The output from the executed plugin is sent back to the Gemini model, which uses this result to formulate a final, human-readable text response.Response to Client: The final text reply is sent back to the user as a JSON object.</details>üõ†Ô∏è Tech Stack & ToolsCategoryTechnologyLanguageTypeScriptFrameworkNode.js with ExpressAI ModelGoogle Gemini 1.5 ProEmbeddingsGoogle text-embedding-004Vector SearchCustom in-memory Cosine SimilarityDev Runnertsx (for fast, modern execution)üöÄ Getting StartedFollow these steps to set up and run the project on your local machine.PrerequisitesNode.js (v18 or later recommended)npm (or your preferred package manager)A Google AI Studio API KeyInstallation & SetupClone the repository:git clone https://github.com/HackManX/Pluggable-AI-Agent-Server.git
cd Pluggable-AI-Agent-Server
Install dependencies:npm install
Set up environment variables:Create a .env file in the project root and add your API key:GEMINI_API_KEY=your-google-ai-studio-api-key-goes-here
Run the development server:The server will start with live-reloading on http://localhost:3000.npm run dev
‚öôÔ∏è API Usage & ExamplesThe agent exposes a single endpoint: POST /agent/message.Request Body{
  "message": "Your question for the agent",
  "session_id": "a-unique-id-for-your-conversation"
}
Example Requests<details><summary><strong>Test with curl (for macOS/Linux/WSL)</strong></summary>Test the Weather Plugin:curl -X POST http://localhost:3000/agent/message \
-H "Content-Type: application/json" \
-d '{
  "message": "What is the weather in Bangalore?",
  "session_id": "session-weather-test"
}'
Test the Math Plugin:curl -X POST http://localhost:3000/agent/message \
-H "Content-Type: application/json" \
-d '{
  "message": "what is 50 * 4 + 10?",
  "session_id": "session-math-test"
}'
</details><details><summary><strong>Test with PowerShell (for Windows)</strong></summary>Test the Weather Plugin:Invoke-WebRequest -Uri http://localhost:3000/agent/message -Method POST -Headers @{"Content-Type"="application/json"} -Body '{"message": "what is the weather in bangalore?", "session_id": "session-weather-test"}'
Test the RAG System:Invoke-WebRequest -Uri http://localhost:3000/agent/message -Method POST -Headers @{"Content-Type"="application/json"} -Body '{"message": "According to the documents, who created Markdown?", "session_id": "session-rag-test"}'
</details>
